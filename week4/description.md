
# ğŸ§  Study Buddy â€” An Interactive Retrieval-Augmented AI Agent

**Study Buddy** is a Retrieval-Augmented Generation (RAG)-based AI assistant designed to help students engage with their study material intelligently. Instead of generating answers from scratch, it smartly searches your notes and delivers relevant, context-rich answers based on what *youâ€™ve already studied*.

This repository is part of a broader AI Agents initiative, focusing on **RAG pipelines, prompt engineering, fallback mechanisms, and confidence-aware LLM responses**.

---

## ğŸ¯ Project Objective

The primary goal is to build an interactive question-answering agent that:
- Retrieves relevant information from your **personal notes**
- Uses LLMs like OpenAI's GPT to construct high-quality, context-aware answers
- Handles a wide variety of questions: direct fact-based, reasoning-based, or general knowledge
- Incorporates ethical and fallback mechanisms to ensure safe and accurate responses
- Scores its own responses using both **heuristic** and **LLM-based** confidence estimation

---

## ğŸ’¡ Core Features

### ğŸ“„ PDF-to-Vector Indexing
- Notes (PDFs) placed in a directory are automatically:
  - Loaded using `PDFPlumber`
  - Split into chunks using `RecursiveCharacterTextSplitter`
  - Embedded via OpenAI Embeddings API
  - Stored in a vector store using **ChromaDB**

### ğŸ§  Multi-Strategy Query Routing
- Questions are classified into types such as:
  - `"rag"` â†’ Uses your notes
  - `"math"` â†’ Uses structured reasoning
  - `"wikipedia"` / `"tavily"` â†’ Uses external search
  - `"fallback"` â†’ Uses LLMâ€™s general knowledge

### ğŸ” Retrieval-Augmented Generation (RAG)
- When a user asks a question, the system:
  - Embeds the query
  - Searches the vector store for top-k similar chunks
  - Constructs a prompt that includes both question + retrieved notes
  - Sends this to OpenAI's Chat API for answer generation
    
### âœ… Confidence Evaluation
- After a response is generated, the system:
  - Computes a **heuristic confidence score** using:
    - Length, keyword overlap, semantic similarity, and presence of errors
  - Optionally asks the **LLM to rate itself** on accuracy and clarity
  - Combines both into a final confidence score

### ğŸ›¡ï¸ Ethical Guardrails
- Every user query and LLM response is checked for:
  - Harmful content
  - Ethically unsafe questions
- Users are warned or blocked when necessary

### ğŸ” Self-Correction
- If the confidence score is low:
  - The system attempts to **self-correct** the response using a refined prompt
  - If that fails, it falls back to general LLM knowledge

---

## ğŸ“ Repository Structure

```
code/
â”œâ”€â”€ main.py # Main CLI app for interactive Q&A
â”œâ”€â”€ rag.py # Document chunking, embedding, and retrieval
â”œâ”€â”€ call_llm.py # Unified interface to call OpenAI Chat API
â”œâ”€â”€ classify_query.py # Classifies question type (RAG, math, etc.) and re-structure query
â”œâ”€â”€ chat_history.py # Manages in-memory chat history and summarization
â”œâ”€â”€ prompt_design.py # Builds domain-aware prompts and instructions
â”œâ”€â”€ ethical_guardrail.py # Performs query and response compliance checks
â”œâ”€â”€ reasoning.py # Math reasoning, fallback strategy, self-correction
â”œâ”€â”€ tools.py # Interfaces to Tavily and Wikipedia search
```
---

## ğŸ› ï¸ How to Set Up

### 1. Clone the Repository
```
git clone https://github.com/DeblinaB1802/INTRO-TO-AI-AGENT.git
cd INTRO-TO-AI-AGENT/week4/code
```

### 2. Install Dependencies
```
pip install -r requirements.txt
```
If requirements.txt is not available, make sure to install:

langchain, chromadb, openai, pdfplumber, sentence-transformers, etc.

### 3. Prepare Your Study Notes

Place all your PDF files in the following folder:
```
notes/
```
The system will automatically parse, chunk, embed, and store them.

### 4. Set Your API Key
You can set your OpenAI and Tavily API key and url using an environment variable or hardcode it in the script (already present for testing):
```
export OPENAI_API_KEY="sk-..."
```
### â–¶ï¸ How to Run
Launch the Study Buddy CLI interface:
```
python main.py
```
You'll be prompted to enter your:
Educational level (used to adapt answer style)
Question

Then you'll receive a curated, AI-generated response based on:
Your personal notes
External knowledge (if needed)

---

## ğŸ§­ Study Buddy â€” Full Execution Flow
### ğŸ” Step-by-step Workflow
#### 1. Startup & Initialization
 - `prep_ragdb()` â†’ Loads PDF notes, chunks them, creates embeddings, stores them in Chroma.
 - `embed_domains()` â†’ Embeds domain-specific labels (used if RAG fallback is needed).
 - `load_chat_history()` â†’ Loads prior interactions for context continuity.
 - `get_style_conditioned_prompt(user_level)` â†’ Adapts tone/complexity based on user level (e.g.,      high_school, postgraduate).

#### 2. User Input Loop
Prompt:
```
Enter your question (or type 'exit' to quit):
```
#### 3. Ethical Check (Guardrail)
`check_ethical_compliance(question, query=True)`
â†’ If unethical, display a warning and skip processing.

#### 4. Context Construction
 - `add_to_history(role="user", content=question)`
 - `extract_entities(question)`

 - `get_optimized_context(question, max_tokens=2000)`
 - â†’ Uses past chat to build context.

 - If empty, uses `summarize_history()` as a fallback.

#### 5. Query Structuring & Classification
 - `structure_query()` â†’ Adds context to rephrase/refine user query.
 - `classify_query()` â†’ Categorizes query into one of:
      - `"rag"`
      - `"math"`
      - `"wikipedia"`
      - `"tavily"`
      - (anything else â†’ fallback)

### ğŸ”„ Branching Logic Based on Query Type
#### - Query Type: `rag`
 - `retrieve_chunks()` â†’ Fetch top-k chunks from vector DB
 - Construct prompt with retrieved notes
 - `call_openai()` â†’ LLM responds based only on notes
 - If keywords like `error`, `not found`, etc. are detected:
 - Build domain-specific prompt using `build_domain_specific_prompt()`
 - Retry `call_openai()` with new prompt

#### - Query Type: `math`
 - `plan_execute_refine_math()` â†’ Handles multi-step math reasoning
 - No RAG or external search involved

#### - Query Type: `wikipedia`
 - `search_wikipedia(structured_query)` â†’ Retrieves Wikipedia summary
 - Prompt LLM with the wiki content
 - call_openai() â†’ Generates response

#### - Query Type: `tavily`
 - `search_tavily(structured_query)` â†’ Uses Tavily API or similar external tool
 - Prompt LLM with the Tavily response
 - `call_openai()` â†’ Generates response

#### - Query Type: Other (Fallback)
 - `fallback_strategy()` â†’ Uses LLM with no external knowledge
 - Prompt is: "Answer using general knowledge..."

### âœ… Post-Answer Evaluation
#### 6. Confidence Evaluation
 - `evaluate_confidence(query, response, context, use_llm=True)`
   â†’ Combines:
 - Heuristic score (based on length, overlap, semantic sim, error terms)
 - LLM self-evaluation score

#### 7. Self-Correction Logic
 - If `final_confidence < 0.7`:
 - Try `self_correct_response()` with rephrased prompt
 - Re-evaluate
 - If still low confidence, use `fallback_strategy()`

#### 8. Logging & History
 - `add_to_history(role="assistant", content=corrected_answer)`
 - Re-check ethical safety of LLM response
 - Display final answer with:
```
***ANSWER***
<final_answer>
```

---

## ğŸŒ High-Level Execution Paths
Hereâ€™s a simplified tree-style flow chart for clarity:

```
Start
 â””â”€â”€ load notes, vectors, chat history
      â””â”€â”€ user input â†’ ethical check
           â””â”€â”€ structure & classify query
                â”œâ”€â”€ rag â†’ retrieve_chunks â†’ call LLM
                â”‚        â””â”€â”€ if failed â†’ domain_prompt â†’ call LLM
                â”œâ”€â”€ math â†’ plan_execute_refine_math
                â”œâ”€â”€ wikipedia â†’ search_wikipedia â†’ call LLM
                â”œâ”€â”€ tavily â†’ search_tavily â†’ call LLM
                â””â”€â”€ fallback â†’ call LLM directly
                      â†“
           evaluate_confidence
                â””â”€â”€ low? â†’ self-correct â†’ failed â†’ fallback â†’ call LLM directly â†’ save to history + print final answer
                                   â””â”€â”€ succeeded â†’ save to history + print final answer
```




